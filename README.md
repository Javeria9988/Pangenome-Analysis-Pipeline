# Pangenome-Analysis-Pipeline

## Overview
This pipeline performs a comprehensive pangenome analysis, starting from genome annotation, through pangenome identification, read simulation, SNP calling, and finally, detecting premature stop codons. The pipeline leverages various bioinformatics tools, integrated within a Nextflow workflow to streamline the analysis of multiple sequences.

## Pipeline Structure
The pipeline consists of the following steps:

1. **Annotate:** Annotates the input sequences using prokka.
2. **Panaroo:** Identifies core and accessory genes across the annotated genomes and generates relevant outputs.
3. **DWGSIM:** Simulates error-free reads from the input sequences.
4. **Snippy:** Aligns sequences and calls SNPs based on the simulated reads.
5. **Detect Stop Codons:** Identifies premature stop codons from the SNP data using python code.

## Workflow
![dissertation project](https://github.com/user-attachments/assets/1af319a6-faed-4d59-b67e-201fde1bd684)

## Installation

### Prerequisites
- [Nextflow](https://www.nextflow.io/)
- [Docker](https://www.docker.com/) for containerized execution of processes.

### Setup
Clone the repository and navigate to the directory:
```bash
git clone https://github.com/Javeria9988/pangenome-pipeline.git
cd pangenome-pipeline
```
### Directory structure for running pipeline
my_nextflow_pipeline/
├── nextflow.config           # Configuration file for the Nextflow pipeline
├── main.nf                   # Main Nextflow script 
├── sequences/                # Input sequences directory
│   ├── sample1.fa            # Example sequence file
│   └── sample2.fa            # Example sequence file
└── modules/                  # Directory for Nextflow modules
    ├── annotate.nf           # Nextflow script for annotate module
    ├── panaroo.nf            # Nextflow script for panaroo module
    ├── dwgsim.nf             # Nextflow script for dwgsim module
    ├── snippy.nf             # Nextflow script for snippy module
    └── detectStopCodons.nf   # Nextflow script for detectStopCodons module

### Usage
### Running the Pipeline
To run the pipeline, use the following command:
nextflow run main.nf -c nextflow.config -resume

### Parameters
params.sequences: Path to the input sequences in .fa format. The default is set to 'sequences/*.fa'.

### Input Data
Sequences: The pipeline expects input sequences in .fa format, located in the directory specified by the params.sequences parameter.

### Output Data
Annotated Genomes: GFF files generated by the annotation step.
Panaroo Outputs: Core and accessory gene information in CSV and FASTA formats.
Simulated Reads: Error-free reads generated by DWGSIM.
SNPs: SNP calling results from Snippy.
Stop Codon Detection: A report detailing any detected premature stop codons in each sample.

## Docker Integration
This pipeline utilizes Docker containers to ensure reproducibility and consistency across different environments. Each step of the pipeline is associated with a specific Docker container, as outlined below:

### Containers Used:

1. **Annotate:**
   - **Container:** `staphb/prokka:latest`
   - **Description:** This container includes Prokka, a tool used for rapid annotation of prokaryotic genomes.

2. **Panaroo:**
   - **Container:** `staphb/panaroo:latest`
   - **Description:** This container includes Panaroo, a tool for pangenome analysis, particularly focused on bacterial genomes.

3. **DWGSIM:**
   - **Container:** `biocontainers/dwgsim:v0.1.12-2-deb_cv1`
   - **Description:** This container includes DWGSIM, a tool for simulating error-free reads from genomic sequences.

4. **Snippy:**
   - **Container:** `staphb/snippy:4.6.0`
   - **Description:** This container includes Snippy, a tool for rapid bacterial SNP calling and variant detection.

5. **Detect Stop Codons:**
   - **Container:** `python-biopython-pandas:latest`
   - **Description:** This custom container includes Python along with the Biopython and Pandas libraries, used for detecting premature stop codons and processing genomic data.

### Usage
To ensure that Docker is enabled, the pipeline is configured with Docker integration:

```groovy
docker {
    enabled = true
}
